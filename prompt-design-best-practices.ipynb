{
  "cells": [
    {
      "cell_type": "code",
      "id": "cziTqqVpVgCTyoWIcTD43ZM3",
      "metadata": {
        "tags": [],
        "id": "cziTqqVpVgCTyoWIcTD43ZM3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997247864,
          "user_tz": 180,
          "elapsed": 4341,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "#instal sdk\n",
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "qJG7QD8L34vz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997770920,
          "user_tz": 180,
          "elapsed": 7286,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "qJG7QD8L34vz",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get project by CLI\n",
        "PROJECT = !gcloud config get-value project\n",
        "PROJECT_ID = PROJECT[0]\n",
        "REGION = \"us-central1\"\n",
        "# start vertex\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "W3FvM4Hr5juN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997773207,
          "user_tz": 180,
          "elapsed": 894,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "W3FvM4Hr5juN",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a generative model gimini pro\n",
        "model = GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "m2_JCp5O5z-J",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997802444,
          "user_tz": 180,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "m2_JCp5O5z-J",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "08ttKwKu56XH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997812736,
          "user_tz": 180,
          "elapsed": 431,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "08ttKwKu56XH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8WiLsMU5-eB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997832076,
          "user_tz": 180,
          "elapsed": 3169,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9bcfeac8-2306-4e4a-c621-42b2984c3cc7"
      },
      "id": "T8WiLsMU5-eB",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"turns\": [\n",
            "    {\n",
            "      \"speaker\": \"Customer\",\n",
            "      \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Restaurant employee\",\n",
            "      \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Customer\",\n",
            "      \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Restaurant employee\",\n",
            "      \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qmIuqGo6DPP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997850380,
          "user_tz": 180,
          "elapsed": 1661,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5f085350-b739-4cf1-a6ba-695b710e37c2"
      },
      "id": "5qmIuqGo6DPP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"drinks\": [\n",
            "    {\n",
            "      \"item\": \"orange juice\",\n",
            "      \"size\": \"small\",\n",
            "      \"quantity\": 1\n",
            "    }\n",
            "  ],\n",
            "  \"food\": [\n",
            "    {\n",
            "      \"item\": \"cheeseburger\",\n",
            "      \"quantity\": 1\n",
            "    },\n",
            "    {\n",
            "      \"item\": \"fries\",\n",
            "      \"size\": \"large\",\n",
            "      \"quantity\": 1,\n",
            "      \"modifier\": \"ketchup on the side\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "6762SoIM6KpQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997879232,
          "user_tz": 180,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "6762SoIM6KpQ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aJqWBK96MwA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997895408,
          "user_tz": 180,
          "elapsed": 7781,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e5c113ad-f7a2-41cb-fa74-a5511bdfd6b1"
      },
      "id": "5aJqWBK96MwA",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Monstera Deliciosa Care Guide\n",
            "\n",
            "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular houseplant known for its large, beautiful leaves with natural holes. It's a relatively low-maintenance plant, but here's a guide to help you keep your Monstera happy and thriving:\n",
            "\n",
            "**Light:**\n",
            "\n",
            "* **Bright indirect light:** This is crucial for healthy growth. Place your Monstera near a window that receives plenty of bright, indirect sunlight. Avoid direct sunlight, as it can scorch the leaves.\n",
            "* **Supplement with artificial light:** If natural light is limited, consider using artificial grow lights to supplement.\n",
            "\n",
            "**Watering:**\n",
            "\n",
            "* **Water thoroughly when the top soil feels dry:** This usually means watering every 7-10 days. Don't let the soil dry out completely, but avoid overwatering as well.\n",
            "* **Check the soil moisture:** The best way to determine when to water is to stick your finger into the top few inches of the soil. If it feels dry, it's time to water.\n",
            "* **Empty excess water:** Ensure the pot has drainage holes to prevent the roots from sitting in water.\n",
            "\n",
            "**Humidity:**\n",
            "\n",
            "* **Enjoys moderate to high humidity:** Monsteras thrive in humid environments. You can increase humidity by placing the pot on a pebble tray filled with water, grouping plants together, or using a humidifier.\n",
            "\n",
            "**Temperature:**\n",
            "\n",
            "* **Prefers warm temperatures:** Aim for temperatures between 65-80°F (18-27°C). Avoid cold drafts and sudden temperature fluctuations.\n",
            "\n",
            "**Soil:**\n",
            "\n",
            "* **Well-draining potting mix:** Use a loose, well-draining potting mix that allows for good aeration and water drainage.\n",
            "\n",
            "**Fertilizer:**\n",
            "\n",
            "* **Fertilize during the growing season:** Feed your Monstera with a balanced liquid fertilizer every 4-6 weeks during spring and summer. Avoid fertilizing during the winter months.\n",
            "\n",
            "**Pruning:**\n",
            "\n",
            "* **Prune to control size and shape:** You can prune your Monstera to encourage bushier growth or to control its size. \n",
            "* **Encourage branching:** To promote branching, cut just above a node (the point where a new leaf or stem emerges).\n",
            "\n",
            "**Repotting:**\n",
            "\n",
            "* **Repot when the plant outgrows its current pot:** You'll know it's time to repot when the roots start to grow out of the drainage holes. Choose a pot that's only slightly larger than the previous one.\n",
            "\n",
            "**Common Problems:**\n",
            "\n",
            "* **Yellowing leaves:** This can be caused by overwatering, underwatering, or nutrient deficiency.\n",
            "* **Brown spots on leaves:** This can be caused by sunburn, low humidity, or pests.\n",
            "* **Leggy growth:** This can be caused by insufficient light.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "* **Wipe leaves regularly:** Dust accumulation can block sunlight and affect the plant's health.\n",
            "* **Provide support:** As your Monstera grows, you may need to provide support such as a moss pole or trellis to help it climb.\n",
            "* **Enjoy your beautiful Monstera!** With proper care, your Monstera Deliciosa will reward you with lush, healthy growth and its unique, eye-catching leaves.\n",
            "\n",
            "**Resources:**\n",
            "\n",
            "* https://www.thesill.com/blog/monstera-plant-care\n",
            "* https://www.bhg.com/gardening/houseplants/care/monstera/\n",
            "* https://www.thespruce.com/monstera-deliciosa-plant-care-4122005\n",
            "\n",
            "I hope this guide helps you care for your Monstera Deliciosa successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW7uqmXp6R3Y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997914087,
          "user_tz": 180,
          "elapsed": 5434,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1dfec00f-d71b-48ce-e108-e8d46374bf61"
      },
      "id": "hW7uqmXp6R3Y",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Your monstera deliciosa is here to help you better understand my needs. \n",
            "\n",
            "As a houseplant, I need a few key things to thrive. Let’s go over them quickly so you can ensure I live a long and happy life! :\n",
            "\n",
            "1.  **Light:**  I love bright, indirect light. A spot near a window that gets morning or afternoon sun is perfect. Avoid direct sunlight, as this can scorch my leaves.\n",
            "2.  **Watering:**  I like my soil to be consistently moist, but not soggy. Water me deeply when the top inch of soil feels dry. Be careful not to overwater, as this can lead to root rot. In winter, I need less water, so allow the soil to dry out more between waterings.\n",
            "3.  **Humidity:**  I appreciate a humid environment. If the air is dry, you can increase humidity by placing me on a pebble tray filled with water or by using a humidifier.\n",
            "4.  **Fertilizer:**  I benefit from regular fertilization, especially during the spring and summer growing seasons. Use a balanced houseplant fertilizer and dilute it to half strength according to the instructions on the package.\n",
            "5.  **Pruning:**   Pruning helps to keep me looking my best and encourages new growth. You can prune off any leggy stems or damaged leaves.\n",
            "6.  **Repotting:**   Repot me every 2-3 years in a pot that is one size larger than my current pot. Use a well-draining potting mix that is specifically designed for houseplants.\n",
            "\n",
            "With a little care and attention, I will thrive in your home and bring you years of enjoyment. \n",
            "\n",
            "If you have any questions, please don’t hesitate to ask. I’m happy to help!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include exemples\n",
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yKbKA0c6Zby",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729997946132,
          "user_tz": 180,
          "elapsed": 871,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a05ea78c-8b9e-455e-f246-f6472c818670"
      },
      "id": "1yKbKA0c6Zby",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Likelihood: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b3ba76w6mmH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998005240,
          "user_tz": 180,
          "elapsed": 870,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "149525fc-1d11-4d73-bf9c-d462ae32e379"
      },
      "id": "4b3ba76w6mmH",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Why did the frog get a job at the bank? \n",
            "\n",
            "> He was good with his *mon-eys*. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fallback response\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmdR3Ewx6z_m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998060212,
          "user_tz": 180,
          "elapsed": 875,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ef1b782b-2b53-4c17-d04d-e72c9dc9cb59"
      },
      "id": "EmdR3Ewx6z_m",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I only talk about pottery!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aBu5PQN695w",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998091576,
          "user_tz": 180,
          "elapsed": 2372,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d937eca7-8b65-4319-f6fd-e6efd6b71592"
      },
      "id": "-aBu5PQN695w",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceramic and porcelain are both types of pottery, but they have some key differences. Ceramic is a more general term that refers to any object made from clay that has been hardened by heat. Porcelain is a type of ceramic that is made from a specific type of clay called kaolin, which gives it a white, translucent appearance. Porcelain is also harder and more durable than other types of ceramic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#contextual information\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUusIHTk7EGR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998175540,
          "user_tz": 180,
          "elapsed": 2863,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ebca7333-1f52-42cb-fc65-1a2790841862"
      },
      "id": "pUusIHTk7EGR",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Aisle Numbers:\n",
            "\n",
            "Here's where you can typically find the items you listed:\n",
            "\n",
            "* **Paper plates:** Aisle 4 (Paper and Plastic Products)\n",
            "* **Mustard:** Aisle 3 (Condiments)\n",
            "* **Potatoes:** Aisle 9 (Produce) \n",
            "\n",
            "**Please note:** This is just a general guide. The aisle numbers may vary depending on the specific grocery store you visit. It's always best to check the store directory or ask an employee for assistance if you can't find something. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits — Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables — Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods — Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy — Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat— Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood— Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli— Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices— Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks— Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery— Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages— Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal—Oats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking— Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods — Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care— Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care— Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies—Laundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items— Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care— Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agpI8q497WlB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998192449,
          "user_tz": 180,
          "elapsed": 1921,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5c085df6-593a-4126-fd99-69dfaf148769"
      },
      "id": "agpI8q497WlB",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Michael's Grocery Store Aisle Numbers:\n",
            "\n",
            "Based on your provided context:\n",
            "\n",
            "* **Paper plates:** You can find paper plates in **Aisle 17 (Household & Cleaning Supplies)**. \n",
            "* **Mustard:** You can find mustard in **Aisle 8 (Condiments & Spices)**.\n",
            "* **Potatoes:** You can find potatoes in two locations:\n",
            "    * **Aisle 2 (Vegetables)** if you're looking for fresh potatoes.\n",
            "    * **Aisle 14 (Frozen Foods)** if you're looking for frozen potatoes. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## structure prompts with prefixes or tags\n",
        "\n",
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQRAAPjh7b3X",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998214219,
          "user_tz": 180,
          "elapsed": 2419,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "aab48dc4-6af0-4ab2-eeb3-929598572ac0"
      },
      "id": "xQRAAPjh7b3X",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allison, I think I may have found your perfect match! His name is Felix. Not only does he share your love of classical music, especially Beethoven, but he's also an excellent cook who specializes in German cuisine, just like you! Plus, he used to be a water polo player, so he's athletic and loves being by the water, just like you enjoy swimming. It sounds like you two could have amazing conversations about music, share delicious meals together, and even enjoy fun beach outings. You should definitely check him out! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# system instructions\n",
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwp5-kpN7ooI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998285597,
          "user_tz": 180,
          "elapsed": 6015,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a15e176b-49c1-49a3-8180-175c29cd8bc0"
      },
      "id": "mwp5-kpN7ooI",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh my, what a question for a music enthusiast like myself!  There's a whole world of incredible musicians worth studying, each offering unique insights into the power and beauty of music. \n",
            "\n",
            "Where should we start?  \n",
            "\n",
            "* **Classical fans** could dive into the complex polyphony of **Johann Sebastian Bach**, the dramatic symphonies of **Ludwig van Beethoven**, or the groundbreaking operas of **Wolfgang Amadeus Mozart**. \n",
            "\n",
            "* Want to explore the evolution of **jazz**?  Listen closely to the improvisational genius of **Louis Armstrong**, the bebop mastery of **Charlie Parker**, or the smooth sounds of **Miles Davis**.\n",
            "\n",
            "*  Interested in the roots of **rock and roll**?  You'll want to get acquainted with pioneers like **Chuck Berry**, **Little Richard**, and **Sister Rosetta Tharpe**.  \n",
            "\n",
            "*  And don't forget the incredible women in music!  From the soulful vocals of **Aretha Franklin** to the songwriting prowess of **Carole King** and the boundary-breaking performances of **Beyoncé**, their contributions are essential listening for any music lover.\n",
            "\n",
            "These are just a tiny fraction of the artists who deserve your attention!  Tell me, what genres get your toes tapping?  We can delve into some specific recommendations together! 😄 🎶 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chain-of-Thought\n",
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKcfCRrD7yUF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998323817,
          "user_tz": 180,
          "elapsed": 2564,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4873cfa9-292f-42c3-9d22-2a5508fde606"
      },
      "id": "wKcfCRrD7yUF",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "Tomorrow's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day (Reconfigured factory)\n",
            "* Low efficiency factories: 1 factory * 30 units/day/factory * 0.5 = 15 units/day (Outaged factory)\n",
            "* **Total production today: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Break down complex tasks\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLpHYSuD78xA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998359409,
          "user_tz": 180,
          "elapsed": 5382,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c030d3cd-3523-4656-9df3-a7d637178233"
      },
      "id": "DLpHYSuD78xA",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## TPUs vs. GPUs: 5 Metaphors\n",
            "\n",
            "1. **TPUs are like data center sprinters, while GPUs are marathon runners.** TPUs excel at specific tasks with high processing power, making them ideal for training deep learning models. GPUs, with their sustained performance, are better suited for tasks requiring long-term computation, like scientific simulations.\n",
            "\n",
            "2. **TPUs are specialized racing cars, while GPUs are powerful SUVs.** TPUs are custom-built for machine learning, offering unparalleled speed and efficiency for specific tasks. GPUs are versatile tools capable of handling various computing needs, but with less specialization and efficiency compared to TPUs.\n",
            "\n",
            "3. **TPUs are like laser scalpels, while GPUs are like Swiss army knives.** TPUs are highly focused on deep learning, making them precise and efficient for specific tasks. GPUs are versatile with broader capabilities but less accuracy and efficiency than TPUs in their domain.\n",
            "\n",
            "4. **TPUs are like a team of specialized doctors, while GPUs are like a single general practitioner.** TPUs utilize multiple custom-designed cores, each optimized for specific machine learning tasks. GPUs have fewer, more versatile cores, making them well-suited for diverse computing needs but less efficient for specialized tasks compared to TPUs.\n",
            "\n",
            "5. **TPUs are like a Formula 1 pit crew, while GPUs are like a mechanic at a local garage.** TPUs excel in parallel processing, utilizing multiple cores to quickly train machine learning models. GPUs, while powerful, handle tasks sequentially, making them slower for training large models compared to TPUs.\n",
            "\n",
            "\n",
            "These metaphors highlight the core differences between TPUs and GPUs: TPUs are highly specialized for deep learning, offering unmatched speed and efficiency in their domain, while GPUs are versatile tools capable of handling diverse computing needs but with less specialization and efficiency compared to TPUs in their areas of expertise.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgG69GRz8ELb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998386209,
          "user_tz": 180,
          "elapsed": 4088,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "24bff11e-5599-45ec-ad42-b1454c623a32"
      },
      "id": "wgG69GRz8ELb",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Pit Crew vs. Mechanic Metaphor: Visualizing the Difference Between TPUs and GPUs\n",
            "\n",
            "From a college student's perspective, the **Formula 1 pit crew versus local garage mechanic** metaphor offers a clear and compelling visual to understand the key differences between TPUs and GPUs. Imagine this:\n",
            "\n",
            "**TPUs:**  A Formula 1 pit crew, expertly trained and equipped with specialized tools. Each member has a specific role, working in perfect synchronicity to achieve a singular goal: a lightning-fast pit stop. They are highly efficient, completing complex tasks with astonishing speed and precision. \n",
            "\n",
            "**GPUs:**  A skilled mechanic at a local garage, capable of handling various repairs and maintenance tasks. They have a broad range of tools and knowledge, making them adaptable to diverse situations. However, due to the wider scope of their expertise, they may not be as fast or specialized as the pit crew for a specific task.\n",
            "\n",
            "This analogy effectively captures the essence of both technologies:\n",
            "\n",
            "- **TPUs:** Focused on parallel processing, utilizing multiple cores to accelerate specific tasks, particularly in the domain of deep learning. They excel in speed and efficiency, similar to the swiftness and precision of the pit crew.\n",
            "- **GPUs:** Versatile tools, capable of handling diverse computing needs, but with less specialization compared to TPUs. They offer broader capabilities, akin to the mechanic's adaptability, but may not achieve the same level of efficiency when dealing with specific tasks.\n",
            "\n",
            "This visual comparison serves as a powerful learning tool for students, helping them grasp the fundamental differences between TPUs and GPUs. It highlights the strengths and limitations of each technology, providing a clear understanding of their respective roles in the world of computing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDQtHDkO8JkY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729998411593,
          "user_tz": 180,
          "elapsed": 5115,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fb0cac8e-cb4f-44a7-fcba-f3686c40630f"
      },
      "id": "aDQtHDkO8JkY",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Pit Stop to Machine Learning Acceleration: Understanding TPUs and GPUs\n",
            "\n",
            "Ever watched a Formula 1 pit stop? It's a breathtaking display of teamwork and precision. In mere seconds, a team of experts equipped with specialized tools refuels the car, changes tires, and sends it back on the track – all in a perfectly synchronized ballet of motion.\n",
            "\n",
            "Now, imagine that pit crew as a **Tensor Processing Unit (TPU)** – a custom-designed machine learning accelerator built for speed and efficiency. Just like the pit crew focuses on a singular goal, the TPU excels at specific deep learning tasks, utilizing multiple cores to achieve lightning-fast computation. \n",
            "\n",
            "But what about your local garage mechanic? They may not have the high-tech tools of a Formula 1 pit crew, but they possess a diverse skillset and can handle a wide range of repairs. This versatility is akin to the **Graphics Processing Unit (GPU)** – a powerful tool capable of handling various computing tasks. While GPUs may not be as specialized as TPUs for specific needs, their broader capabilities make them adaptable to diverse situations.\n",
            "\n",
            "So, the next time you're watching a Formula 1 race, think of the pit crew as a TPU – a powerhouse of parallel processing designed for specific tasks. And when you need your car fixed, remember that your local mechanic, like a GPU, offers a wide range of capabilities to tackle different challenges.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-01-779130338041 (Oct 26, 2024, 11:44:16 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}